# Survey of iterate averaging methods

Deep Neural Networks (DNN) achieve state of the art performance in a wide range of problems. Given their impressive performance, researchers are particularly interested in their optimization, and one of the simplest optimization algorithms contributing in the success of DNN models is the Stochastic Gradient Descent (SGD). While its theoretical properties make it the ideal optimization approach for large scale applications, it has been observed that SGD suffers from the effect of noisy gradient estimates. In this project we are interested in iterate averaging methods as a noise reduction tools to improve SGD results.



The file `Iterate_Averaging.ipynb` can be opened using Google Colab where all necessary librairies and dependencies are available and ready to be used.
The code can be simply ran cell by cell.
